# efetch_data_to_dune.py
"""
Ğ Ğ¾Ğ·ÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ±Ñ–Ñ€ Ğ´Ğ°Ğ½Ğ¸Ñ… Ğ· Extended API Ğ´Ğ»Ñ ÑƒĞ½Ñ–ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ñƒ Ğ±Ñ–Ñ€Ğ¶Ñ–
Ğ—Ğ±Ğ¸Ñ€Ğ°Ñ”:
1. Markets data (Ñ†Ñ–Ğ½Ğ¸, Ğ¾Ğ±ÑÑĞ³Ğ¸, open interest) - Ğ· Ñ–ÑÑ‚Ğ¾Ñ€Ñ–Ñ”Ñ
2. Trading statistics (Ñ–ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡Ğ½Ñ– Ğ¾Ğ±ÑÑĞ³Ğ¸) - Ğ· Ñ–ÑÑ‚Ğ¾Ñ€Ñ–Ñ”Ñ  
3. TVL Ğ´Ğ°Ğ½Ñ– Ğ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ĞºÑ‚Ñ–Ğ² (Ñ‡ĞµÑ€ĞµĞ· DeFiLlama API) - Ğ· Ñ–ÑÑ‚Ğ¾Ñ€Ñ–Ñ”Ñ
4. On-chain metrics (Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ»Ğ¾ĞºÑ‡ĞµĞ¹Ğ½ API) - Ğ½Ğ¾Ğ²Ğ° Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ
Ğ—Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ” Ğ²ÑĞµ Ğ² Ğ¾ĞºÑ€ĞµĞ¼Ñ– CSV Ñ„Ğ°Ğ¹Ğ»Ğ¸ Ğ´Ğ»Ñ Dune Analytics Ğ· timestamps
"""

import requests
import pandas as pd
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import json
import time

# === ĞšĞĞĞ¤Ğ†Ğ“Ğ£Ğ ĞĞ¦Ğ†Ğ¯ ===
UPLOADS_DIR = "uploads"
TIMEOUT = 30
MAX_RETRIES = 3

# API endpoints Ğ´Ğ»Ñ Ñ€Ñ–Ğ·Ğ½Ğ¸Ñ… Ğ¼ĞµÑ€ĞµĞ¶
ENDPOINTS = {
    'ethereum': {
        'markets': 'https://api.extended.exchange/api/v1/info/markets',
        'trading': 'https://api.extended.exchange/api/v1/exchange/stats/trading',
        'start_date': '2025-03-11',
        'contract': '0x1cE5D7f52A8aBd23551e91248151CA5A13353C65'
    },
    'starknet': {
        'markets': 'https://api.starknet.extended.exchange/api/v1/info/markets', 
        'trading': 'https://api.starknet.extended.exchange/api/v1/exchange/stats/trading',
        'start_date': '2025-08-10',
        'contract': '0x062da0780fae50d68cecaa5a051606dc21217ba290969b302db4dd99d2e9b470'
    }
}

# DeFiLlama TVL endpoint
DEFILLAMA_TVL_API = "https://api.llama.fi/protocol/extended"

def ensure_uploads_dir():
    """Ğ¡Ñ‚Ğ²Ğ¾Ñ€ÑÑ” Ğ¿Ğ°Ğ¿ĞºÑƒ uploads ÑĞºÑ‰Ğ¾ Ñ—Ñ— Ğ½ĞµĞ¼Ğ°Ñ”"""
    os.makedirs(UPLOADS_DIR, exist_ok=True)
    print("ğŸ“ Uploads directory ready")

def retry_request(func, *args, **kwargs):
    """Retry mechanism Ğ´Ğ»Ñ API Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñ–Ğ²"""
    for attempt in range(MAX_RETRIES):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            if attempt == MAX_RETRIES - 1:
                raise e
            print(f"âš ï¸ Attempt {attempt + 1} failed: {e}")
            time.sleep(2 ** attempt)

def fetch_markets_data(chain: str) -> List[Dict]:
    """
    ĞÑ‚Ñ€Ğ¸Ğ¼ÑƒÑ” Ğ´Ğ°Ğ½Ñ– Ñ€Ğ¸Ğ½ĞºÑ–Ğ² (markets) Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ñ— Ğ¼ĞµÑ€ĞµĞ¶Ñ–
    ĞŸĞ¾Ğ²ĞµÑ€Ñ‚Ğ°Ñ” ÑĞ¿Ğ¸ÑĞ¾Ğº Ñ€Ğ¸Ğ½ĞºÑ–Ğ² Ğ· Ñ†Ñ–Ğ½Ğ°Ğ¼Ğ¸, Ğ¾Ğ±ÑÑĞ³Ğ°Ğ¼Ğ¸ Ñ‚Ğ° open interest
    """
    try:
        url = ENDPOINTS[chain]['markets']
        print(f"ğŸ”„ Fetching markets data for {chain}...")
        
        def make_request():
            response = requests.get(url, timeout=TIMEOUT)
            response.raise_for_status()
            return response.json()
        
        data = retry_request(make_request)
        markets = data.get("data", []) if isinstance(data, dict) else data
        
        print(f"âœ… Got {len(markets)} markets from {chain}")
        return markets or []
        
    except Exception as e:
        print(f"âŒ Error fetching {chain} markets: {e}")
        return []

def fetch_trading_stats(chain: str, date: str) -> Dict:
    """
    ĞÑ‚Ñ€Ğ¸Ğ¼ÑƒÑ” ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ñ‚Ğ¾Ñ€Ğ³Ñ–Ğ² Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ñ— Ğ´Ğ°Ñ‚Ğ¸ Ñ‚Ğ° Ğ¼ĞµÑ€ĞµĞ¶Ñ–
    date Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚: YYYY-MM-DD
    """
    try:
        url = f"{ENDPOINTS[chain]['trading']}?fromDate={date}&toDate={date}"
        print(f"ğŸ”„ Fetching trading stats for {chain} on {date}...")
        
        def make_request():
            response = requests.get(url, timeout=TIMEOUT)
            response.raise_for_status()
            return response.json()
        
        data = retry_request(make_request)
        
        # ĞĞ±Ñ€Ğ¾Ğ±Ğ»ÑÑ”Ğ¼Ğ¾ Ğ´Ğ°Ğ½Ñ– ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸
        if isinstance(data, dict) and 'data' in data:
            trading_data = data['data']
            if trading_data:
                total_volume = sum(float(item.get('tradingVolume', 0)) for item in trading_data)
                trades_count = len(trading_data)
                return {
                    'date': date,
                    'chain': chain,
                    'daily_volume': total_volume,
                    'trades_count': trades_count,
                    'timestamp': datetime.utcnow().isoformat() + "Z"
                }
        
        return {
            'date': date, 
            'chain': chain, 
            'daily_volume': 0, 
            'trades_count': 0,
            'timestamp': datetime.utcnow().isoformat() + "Z"
        }
        
    except Exception as e:
        print(f"âŒ Error fetching trading stats for {chain}: {e}")
        return {
            'date': date, 
            'chain': chain, 
            'daily_volume': 0, 
            'trades_count': 0,
            'timestamp': datetime.utcnow().isoformat() + "Z"
        }

def fetch_tvl_data() -> Optional[Dict]:
    """
    ĞÑ‚Ñ€Ğ¸Ğ¼ÑƒÑ” TVL Ğ´Ğ°Ğ½Ñ– Ğ· DeFiLlama
    """
    try:
        print("ğŸ”„ Fetching TVL data from DeFiLlama...")
        
        def make_request():
            response = requests.get(DEFILLAMA_TVL_API, timeout=TIMEOUT)
            response.raise_for_status()
            return response.json()
        
        data = retry_request(make_request)
        print("âœ… Got TVL data from DeFiLlama")
        return data
        
    except Exception as e:
        print(f"âŒ Error fetching TVL data: {e}")
        return None

def fetch_onchain_metrics(chain: str) -> Dict:
    """
    Ğ—Ğ±Ğ¸Ñ€Ğ°Ñ” Ğ´Ğ¾Ğ´Ğ°Ñ‚ĞºĞ¾Ğ²Ñ– on-chain Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
    """
    try:
        print(f"ğŸ”„ Fetching on-chain metrics for {chain}...")
        
        # Ğ¢ÑƒÑ‚ Ğ¼Ğ¾Ğ¶Ğ½Ğ° Ğ´Ğ¾Ğ´Ğ°Ñ‚Ğ¸ Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¸ Ğ´Ğ¾ Ğ±Ğ»Ğ¾ĞºÑ‡ĞµĞ¹Ğ½ API
        # ĞŸĞ¾ĞºĞ¸ Ñ‰Ğ¾ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ‚Ğ°Ñ”Ğ¼Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ– Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
        current_time = datetime.utcnow().isoformat() + "Z"
        
        metrics = {
            'timestamp': current_time,
            'chain': chain,
            'contract_address': ENDPOINTS[chain]['contract'],
            # Ğ¢ÑƒÑ‚ Ğ±ÑƒĞ´ÑƒÑ‚ÑŒ Ğ´Ğ¾Ğ´Ğ°Ğ½Ñ– Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ– on-chain Ğ´Ğ°Ğ½Ñ–
            'active_users_24h': 0,  # Ğ‘ÑƒĞ´Ğµ Ñ€ĞµĞ°Ğ»Ñ–Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ¿Ñ–Ğ·Ğ½Ñ–ÑˆĞµ
            'total_transactions': 0,  # Ğ‘ÑƒĞ´Ğµ Ñ€ĞµĞ°Ğ»Ñ–Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ¿Ñ–Ğ·Ğ½Ñ–ÑˆĞµ
        }
        
        return metrics
        
    except Exception as e:
        print(f"âŒ Error fetching on-chain metrics for {chain}: {e}")
        return {}

def normalize_markets_data(markets: List[Dict], chain: str) -> List[Dict]:
    """
    ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ñ–Ğ·ÑƒÑ” Ğ´Ğ°Ğ½Ñ– Ñ€Ğ¸Ğ½ĞºÑ–Ğ² Ğ² ÑƒĞ½Ñ–Ñ„Ñ–ĞºĞ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ· Ğ´Ğ¾Ğ´Ğ°Ğ²Ğ°Ğ½Ğ½ÑĞ¼ timestamp
    """
    rows = []
    fetched_at = datetime.utcnow().isoformat() + "Z"
    
    for market in markets:
        if not isinstance(market, dict):
            continue
            
        # ĞÑ‚Ñ€Ğ¸Ğ¼ÑƒÑ”Ğ¼Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ñ€Ğ¸Ğ½ĞºÑƒ
        stats = market.get("marketStats", {})
        
        # Ğ¡Ñ‚Ğ²Ğ¾Ñ€ÑÑ”Ğ¼Ğ¾ Ğ·Ğ°Ğ¿Ğ¸Ñ Ğ· ÑƒÑÑ–Ğ¼Ğ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ¸Ğ¼Ğ¸
        row = {
            "timestamp": fetched_at,
            "fetched_at": fetched_at,
            "chain": chain,
            "market": market.get("name", "UNKNOWN"),
            
            # Ğ¦Ñ–Ğ½Ğ¸
            "lastPrice": float(stats.get("lastPrice", 0) or 0),
            "bidPrice": float(stats.get("bidPrice", 0) or 0),
            "askPrice": float(stats.get("askPrice", 0) or 0),
            "markPrice": float(stats.get("markPrice", 0) or 0),
            "indexPrice": float(stats.get("indexPrice", 0) or 0),
            
            # ĞĞ±ÑÑĞ³Ğ¸ Ñ‚Ğ° Ñ–Ğ½Ñ‚ĞµÑ€ĞµÑĞ¸
            "dailyVolume": float(stats.get("dailyVolume", 0) or 0),
            "dailyVolumeBase": float(stats.get("dailyVolumeBase", 0) or 0),
            "openInterest": float(stats.get("openInterest", 0) or 0),
            
            # Ğ”Ğ¾Ğ´Ğ°Ñ‚ĞºĞ¾Ğ²Ñ– Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
            "fundingRate": float(stats.get("fundingRate", 0) or 0),
            "priceChange24h": float(stats.get("priceChange24h", 0) or 0),
            
            # Ğ Ğ¾Ğ·Ñ€Ğ°Ñ…Ğ¾Ğ²ÑƒÑ”Ğ¼Ğ¾ spread
            "spread_pct": 0,
        }
        
        # Ğ Ğ¾Ğ·Ñ€Ğ°Ñ…Ğ¾Ğ²ÑƒÑ”Ğ¼Ğ¾ spread ÑĞºÑ‰Ğ¾ Ñ” bid Ñ‚Ğ° ask
        if row["bidPrice"] > 0 and row["askPrice"] > 0 and row["lastPrice"] > 0:
            row["spread_pct"] = (row["askPrice"] - row["bidPrice"]) / row["lastPrice"] * 100
            
        rows.append(row)
    
    return rows

def append_to_csv(df: pd.DataFrame, file_path: str, max_rows: int = 100000):
    """
    Ğ£Ğ½Ñ–Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ´Ğ°Ğ²Ğ°Ğ½Ğ½Ñ Ğ´Ğ°Ğ½Ğ¸Ñ… Ğ´Ğ¾ CSV Ğ· Ğ¾Ğ±Ğ¼ĞµĞ¶ĞµĞ½Ğ½ÑĞ¼ Ñ€Ğ¾Ğ·Ğ¼Ñ–Ñ€Ñƒ
    """
    if os.path.exists(file_path):
        # Ğ§Ğ¸Ñ‚Ğ°Ñ”Ğ¼Ğ¾ Ñ–ÑĞ½ÑƒÑÑ‡Ñ– Ğ´Ğ°Ğ½Ñ–
        try:
            existing = pd.read_csv(file_path)
            print(f"ğŸ“– Found existing file with {len(existing)} rows")
            
            # ĞĞ±'Ñ”Ğ´Ğ½ÑƒÑ”Ğ¼Ğ¾ Ğ· Ğ½Ğ¾Ğ²Ğ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ¸Ğ¼Ğ¸
            combined = pd.concat([existing, df], ignore_index=True)
            
            # Ğ¡Ğ¾Ñ€Ñ‚ÑƒÑ”Ğ¼Ğ¾ Ğ¿Ğ¾ timestamp (Ğ½Ğ¾Ğ²Ñ–ÑˆÑ– Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ·Ğ²ĞµÑ€Ñ…Ñƒ)
            if 'timestamp' in combined.columns:
                combined = combined.sort_values('timestamp', ascending=False)
            elif 'fetched_at' in combined.columns:
                combined = combined.sort_values('fetched_at', ascending=False)
            
            # ĞĞ±Ğ¼ĞµĞ¶ÑƒÑ”Ğ¼Ğ¾ ĞºÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ Ğ·Ğ°Ğ¿Ğ¸ÑÑ–Ğ²
            combined = combined.head(max_rows)
            
            # Ğ—Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ”Ğ¼Ğ¾
            combined.to_csv(file_path, index=False)
            print(f"âœ… Updated {os.path.basename(file_path)}: {len(combined)} total rows")
            
        except Exception as e:
            print(f"âš ï¸ Error reading existing file: {e}")
            df.to_csv(file_path, index=False)
            print(f"âœ… Created new {os.path.basename(file_path)}: {len(df)} rows")
    else:
        # Ğ¡Ñ‚Ğ²Ğ¾Ñ€ÑÑ”Ğ¼Ğ¾ Ğ½Ğ¾Ğ²Ğ¸Ğ¹ Ñ„Ğ°Ğ¹Ğ»
        df.to_csv(file_path, index=False)
        print(f"âœ… Created {os.path.basename(file_path)}: {len(df)} rows")

def save_markets_data(df: pd.DataFrame):
    """Ğ—Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ” Ğ´Ğ°Ğ½Ñ– Ñ€Ğ¸Ğ½ĞºÑ–Ğ² Ğ· Ñ–ÑÑ‚Ğ¾Ñ€Ñ–Ñ”Ñ"""
    file_path = os.path.join(UPLOADS_DIR, "extended_markets_data.csv")
    append_to_csv(df, file_path, max_rows=50000)

def save_trading_stats(stats_list: List[Dict]):
    """Ğ—Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ” ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ñ‚Ğ¾Ñ€Ğ³Ñ–Ğ² Ğ· Ñ–ÑÑ‚Ğ¾Ñ€Ñ–Ñ”Ñ"""
    if not stats_list:
        print("âš ï¸ No trading stats to save")
        return
    
    df = pd.DataFrame(stats_list)
    file_path = os.path.join(UPLOADS_DIR, "extended_trading_stats.csv")
    
    # Ğ”Ğ»Ñ trading stats Ğ¿ĞµÑ€ĞµĞ²Ñ–Ñ€ÑÑ”Ğ¼Ğ¾ Ğ´ÑƒĞ±Ğ»Ñ–ĞºĞ°Ñ‚Ğ¸ Ğ¿Ğ¾ Ğ´Ğ°Ñ‚Ñ– Ñ‚Ğ° Ğ¼ĞµÑ€ĞµĞ¶Ñ–
    if os.path.exists(file_path):
        try:
            existing = pd.read_csv(file_path)
            existing_keys = set(zip(existing['date'], existing['chain']))
            new_data = [stat for stat in stats_list 
                       if (stat['date'], stat['chain']) not in existing_keys]
            
            if new_data:
                new_df = pd.DataFrame(new_data)
                append_to_csv(new_df, file_path, max_rows=10000)
            else:
                print("â„¹ï¸ No new trading stats to add")
        except Exception as e:
            print(f"âš ï¸ Error processing trading stats: {e}")
            append_to_csv(df, file_path, max_rows=10000)
    else:
        append_to_csv(df, file_path, max_rows=10000)

def save_tvl_data(tvl_data: Dict):
    """Ğ—Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ” TVL Ğ´Ğ°Ğ½Ñ– Ğ· Ñ–ÑÑ‚Ğ¾Ñ€Ñ–Ñ”Ñ Ñ‚Ğ° Ğ¿Ğ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¾Ñ Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ¾Ñ"""
    if not tvl_data:
        print("âš ï¸ No TVL data to save")
        return
        
    current_time = datetime.utcnow().isoformat() + "Z"
    date_str = datetime.utcnow().strftime('%Y-%m-%d')
    
    tvl_records = []
    
    # Ğ—Ğ°Ğ³Ğ°Ğ»ÑŒĞ½Ğ¸Ğ¹ TVL
    total_tvl = float(tvl_data.get('tvl', 0) or 0)
    tvl_records.append({
        'timestamp': current_time,
        'date': date_str,
        'chain': 'total',
        'tvl_usd': total_tvl
    })
    
    # TVL Ğ¿Ğ¾ Ğ¾ĞºÑ€ĞµĞ¼Ğ¸Ñ… Ğ¼ĞµÑ€ĞµĞ¶Ğ°Ñ…
    chain_tvls = tvl_data.get('chainTvls', {})
    for chain, tvl_value in chain_tvls.items():
        chain_clean = chain.lower().replace('-', '_')
        tvl_records.append({
            'timestamp': current_time,
            'date': date_str,
            'chain': chain_clean,
            'tvl_usd': float(tvl_value or 0)
        })
    
    # Ğ”Ğ¾Ğ´Ğ°Ñ”Ğ¼Ğ¾ Ğ¾ĞºÑ€ĞµĞ¼Ğ¾ ethereum Ñ‚Ğ° starknet ÑĞºÑ‰Ğ¾ Ñ—Ñ… Ğ½ĞµĞ¼Ğ°Ñ”
    chains_found = set(rec['chain'] for rec in tvl_records)
    
    if 'ethereum' not in chains_found and 'Ethereum' in chain_tvls:
        tvl_records.append({
            'timestamp': current_time,
            'date': date_str, 
            'chain': 'ethereum',
            'tvl_usd': float(chain_tvls.get('Ethereum', 0) or 0)
        })
        
    if 'starknet' not in chains_found and 'Starknet' in chain_tvls:
        tvl_records.append({
            'timestamp': current_time,
            'date': date_str,
            'chain': 'starknet', 
            'tvl_usd': float(chain_tvls.get('Starknet', 0) or 0)
        })
    
    if tvl_records:
        df = pd.DataFrame(tvl_records)
        file_path = os.path.join(UPLOADS_DIR, "extended_tvl_data.csv")
        append_to_csv(df, file_path, max_rows=20000)

def save_onchain_metrics(metrics_list: List[Dict]):
    """Ğ—Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ” on-chain Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸"""
    if not metrics_list:
        return
        
    df = pd.DataFrame(metrics_list)
    file_path = os.path.join(UPLOADS_DIR, "extended_onchain_metrics.csv")
    append_to_csv(df, file_path, max_rows=10000)

def main():
    """
    Ğ“Ğ¾Ğ»Ğ¾Ğ²Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ - Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ” Ğ²ÑÑ– Ğ´Ğ°Ğ½Ñ– Extended Ğ±Ñ–Ñ€Ğ¶Ñ–
    """
    print("ğŸš€ Starting Extended Exchange data collection...")
    print(f"ğŸ•’ Current time: {datetime.utcnow().isoformat()}Z")
    ensure_uploads_dir()
    
    all_markets_data = []
    trading_stats = []
    onchain_metrics = []
    
    # === 1. Ğ—Ğ‘Ğ˜Ğ ĞĞ„ĞœĞ Ğ”ĞĞĞ† Ğ Ğ˜ĞĞšĞ†Ğ’ ===
    print("\n=== ğŸ“Š COLLECTING MARKETS DATA ===")
    for chain in ['ethereum', 'starknet']:
        markets = fetch_markets_data(chain)
        if markets:
            normalized = normalize_markets_data(markets, chain)
            all_markets_data.extend(normalized)
            
            # Ğ—Ğ±Ğ¸Ñ€Ğ°Ñ”Ğ¼Ğ¾ also on-chain Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
            metrics = fetch_onchain_metrics(chain)
            if metrics:
                onchain_metrics.append(metrics)
    
    if all_markets_data:
        markets_df = pd.DataFrame(all_markets_data)
        save_markets_data(markets_df)
        print(f"ğŸ“Š Processed {len(all_markets_data)} market records")
    
    # === 2. Ğ—Ğ‘Ğ˜Ğ ĞĞ„ĞœĞ Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ£ Ğ¢ĞĞ Ğ“Ğ†Ğ’ ===
    print("\n=== ğŸ“ˆ COLLECTING TRADING STATISTICS ===")
    # Ğ—Ğ±Ğ¸Ñ€Ğ°Ñ”Ğ¼Ğ¾ Ğ´Ğ°Ğ½Ñ– Ğ·Ğ° Ğ¾ÑÑ‚Ğ°Ğ½Ğ½Ñ– 14 Ğ´Ğ½Ñ–Ğ² Ğ´Ğ»Ñ Ğ±Ñ–Ğ»ÑŒÑˆ Ğ¿Ğ¾Ğ²Ğ½Ğ¾Ñ— ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½Ğ¸
    for days_back in range(14):
        date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')
        
        for chain in ['ethereum', 'starknet']:
            # ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ÑÑ”Ğ¼Ğ¾ Ñ‡Ğ¸ Ñ†Ñ Ğ¼ĞµÑ€ĞµĞ¶Ğ° Ğ±ÑƒĞ»Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ½Ğ° Ñ‚Ñƒ Ğ´Ğ°Ñ‚Ñƒ
            start_date = datetime.strptime(ENDPOINTS[chain]['start_date'], '%Y-%m-%d')
            check_date = datetime.strptime(date, '%Y-%m-%d')
            
            if check_date >= start_date:
                stats = fetch_trading_stats(chain, date)
                trading_stats.append(stats)
    
    if trading_stats:
        save_trading_stats(trading_stats)
        print(f"ğŸ“ˆ Processed {len(trading_stats)} trading stat records")
    
    # === 3. Ğ—Ğ‘Ğ˜Ğ ĞĞ„ĞœĞ TVL Ğ”ĞĞĞ† ===
    print("\n=== ğŸ’° COLLECTING TVL DATA ===")
    tvl_data = fetch_tvl_data()
    if tvl_data:
        save_tvl_data(tvl_data)
        print("ğŸ’° Processed TVL data")
    
    # === 4. Ğ—Ğ‘Ğ•Ğ Ğ†Ğ“ĞĞ„ĞœĞ ON-CHAIN ĞœĞ•Ğ¢Ğ Ğ˜ĞšĞ˜ ===
    if onchain_metrics:
        save_onchain_metrics(onchain_metrics)
        print(f"â›“ï¸ Processed {len(onchain_metrics)} on-chain metric records")
    
    # === 5. SUMMARY ===
    print("\n=== âœ… COLLECTION COMPLETED ===")
    print("ğŸ“ Check uploads/ directory for CSV files:")
    
    for filename in os.listdir(UPLOADS_DIR):
        if filename.endswith('.csv'):
            file_path = os.path.join(UPLOADS_DIR, filename)
            file_size = os.path.getsize(file_path)
            with open(file_path, 'r') as f:
                line_count = sum(1 for _ in f) - 1  # -1 Ğ´Ğ»Ñ header
            print(f"  ğŸ“„ {filename}: {line_count} rows, {file_size} bytes")

if __name__ == "__main__":
    main()
