# efetch_data_to_dune.py
"""
–†–æ–∑—à–∏—Ä–µ–Ω–∏–π –∑–±—ñ—Ä –¥–∞–Ω–∏—Ö –∑ Extended API –¥–ª—è —É–Ω—ñ–∫–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –±—ñ—Ä–∂—ñ
–ó–±–∏—Ä–∞—î:
1. Markets data (—Ü—ñ–Ω–∏, –æ–±—Å—è–≥–∏, open interest) - –∑ —ñ—Å—Ç–æ—Ä—ñ—î—é
2. Trading statistics (—ñ—Å—Ç–æ—Ä–∏—á–Ω—ñ –æ–±—Å—è–≥–∏) - –∑ —ñ—Å—Ç–æ—Ä—ñ—î—é  
3. TVL –¥–∞–Ω—ñ –∑ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ñ–≤ (—á–µ—Ä–µ–∑ DeFiLlama API) - –∑ —ñ—Å—Ç–æ—Ä—ñ—î—é
4. On-chain metrics (—á–µ—Ä–µ–∑ –±–ª–æ–∫—á–µ–π–Ω API) - –Ω–æ–≤–∞ —Ñ—É–Ω–∫—Ü—ñ—è
–ó–±–µ—Ä—ñ–≥–∞—î –≤—Å–µ –≤ –æ–∫—Ä–µ–º—ñ CSV —Ñ–∞–π–ª–∏ –¥–ª—è Dune Analytics –∑ timestamps
"""

import requests
import pandas as pd
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import json
import time

# === –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø ===
UPLOADS_DIR = "uploads"
TIMEOUT = 30
MAX_RETRIES = 3

# API endpoints –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –º–µ—Ä–µ–∂
ENDPOINTS = {
    'ethereum': {
        'markets': 'https://api.extended.exchange/api/v1/info/markets',
        'trading': 'https://api.extended.exchange/api/v1/exchange/stats/trading',
        'start_date': '2025-03-11',
        'contract': '0x1cE5D7f52A8aBd23551e91248151CA5A13353C65'
    },
    'starknet': {
        'markets': 'https://api.starknet.extended.exchange/api/v1/info/markets', 
        'trading': 'https://api.starknet.extended.exchange/api/v1/exchange/stats/trading',
        'start_date': '2025-08-10',
        'contract': '0x062da0780fae50d68cecaa5a051606dc21217ba290969b302db4dd99d2e9b470'
    }
}

# DeFiLlama TVL endpoint
DEFILLAMA_TVL_API = "https://api.llama.fi/protocol/extended"

def ensure_uploads_dir():
    """–°—Ç–≤–æ—Ä—é—î –ø–∞–ø–∫—É uploads —è–∫—â–æ —ó—ó –Ω–µ–º–∞—î"""
    os.makedirs(UPLOADS_DIR, exist_ok=True)
    print("üìÅ Uploads directory ready")

def retry_request(func, *args, **kwargs):
    """Retry mechanism –¥–ª—è API –∑–∞–ø–∏—Ç—ñ–≤"""
    for attempt in range(MAX_RETRIES):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            if attempt == MAX_RETRIES - 1:
                raise e
            print(f"‚ö†Ô∏è Attempt {attempt + 1} failed: {e}")
            time.sleep(2 ** attempt)

def fetch_markets_data(chain: str) -> List[Dict]:
    """
    –û—Ç—Ä–∏–º—É—î –¥–∞–Ω—ñ —Ä–∏–Ω–∫—ñ–≤ (markets) –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó –º–µ—Ä–µ–∂—ñ
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ —Ä–∏–Ω–∫—ñ–≤ –∑ —Ü—ñ–Ω–∞–º–∏, –æ–±—Å—è–≥–∞–º–∏ —Ç–∞ open interest
    """
    try:
        url = ENDPOINTS[chain]['markets']
        print(f"üîÑ Fetching markets data for {chain}...")
        
        def make_request():
            response = requests.get(url, timeout=TIMEOUT)
            response.raise_for_status()
            return response.json()
        
        data = retry_request(make_request)
        markets = data.get("data", []) if isinstance(data, dict) else data
        
        print(f"‚úÖ Got {len(markets)} markets from {chain}")
        return markets or []
        
    except Exception as e:
        print(f"‚ùå Error fetching {chain} markets: {e}")
        return []

def fetch_trading_stats(chain: str, date: str) -> Dict:
    """
    –û—Ç—Ä–∏–º—É—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ—Ä–≥—ñ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó –¥–∞—Ç–∏ —Ç–∞ –º–µ—Ä–µ–∂—ñ
    date —Ñ–æ—Ä–º–∞—Ç: YYYY-MM-DD
    """
    try:
        url = f"{ENDPOINTS[chain]['trading']}?fromDate={date}&toDate={date}"
        print(f"üîÑ Fetching trading stats for {chain} on {date}...")
        
        def make_request():
            response = requests.get(url, timeout=TIMEOUT)
            response.raise_for_status()
            return response.json()
        
        data = retry_request(make_request)
        
        # –û–±—Ä–æ–±–ª—è—î–º–æ –¥–∞–Ω—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if isinstance(data, dict) and 'data' in data:
            trading_data = data['data']
            if trading_data:
                total_volume = sum(float(item.get('tradingVolume', 0)) for item in trading_data)
                trades_count = len(trading_data)
                return {
                    'date': date,
                    'chain': chain,
                    'daily_volume': total_volume,
                    'trades_count': trades_count,
                    'timestamp': datetime.utcnow().isoformat() + "Z"
                }
        
        return {
            'date': date, 
            'chain': chain, 
            'daily_volume': 0, 
            'trades_count': 0,
            'timestamp': datetime.utcnow().isoformat() + "Z"
        }
        
    except Exception as e:
        print(f"‚ùå Error fetching trading stats for {chain}: {e}")
        return {
            'date': date, 
            'chain': chain, 
            'daily_volume': 0, 
            'trades_count': 0,
            'timestamp': datetime.utcnow().isoformat() + "Z"
        }

def fetch_tvl_data() -> Optional[Dict]:
    """
    –û—Ç—Ä–∏–º—É—î TVL –¥–∞–Ω—ñ –∑ DeFiLlama
    """
    try:
        print("üîÑ Fetching TVL data from DeFiLlama...")
        
        def make_request():
            response = requests.get(DEFILLAMA_TVL_API, timeout=TIMEOUT)
            response.raise_for_status()
            return response.json()
        
        data = retry_request(make_request)
        print("‚úÖ Got TVL data from DeFiLlama")
        return data
        
    except Exception as e:
        print(f"‚ùå Error fetching TVL data: {e}")
        return None

def fetch_onchain_metrics(chain: str) -> Dict:
    """
    –ó–±–∏—Ä–∞—î –¥–æ–¥–∞—Ç–∫–æ–≤—ñ on-chain –º–µ—Ç—Ä–∏–∫–∏
    """
    try:
        print(f"üîÑ Fetching on-chain metrics for {chain}...")
        
        # –¢—É—Ç –º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –∑–∞–ø–∏—Ç–∏ –¥–æ –±–ª–æ–∫—á–µ–π–Ω API
        # –ü–æ–∫–∏ —â–æ –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –±–∞–∑–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
        current_time = datetime.utcnow().isoformat() + "Z"
        
        metrics = {
            'timestamp': current_time,
            'chain': chain,
            'contract_address': ENDPOINTS[chain]['contract'],
            # –¢—É—Ç –±—É–¥—É—Ç—å –¥–æ–¥–∞–Ω—ñ —Ä–µ–∞–ª—å–Ω—ñ on-chain –¥–∞–Ω—ñ
            'active_users_24h': 0,  # –ë—É–¥–µ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –ø—ñ–∑–Ω—ñ—à–µ
            'total_transactions': 0,  # –ë—É–¥–µ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –ø—ñ–∑–Ω—ñ—à–µ
        }
        
        return metrics
        
    except Exception as e:
        print(f"‚ùå Error fetching on-chain metrics for {chain}: {e}")
        return {}

def normalize_markets_data(markets: List[Dict], chain: str) -> List[Dict]:
    """
    –ù–æ—Ä–º–∞–ª—ñ–∑—É—î –¥–∞–Ω—ñ —Ä–∏–Ω–∫—ñ–≤ –≤ —É–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç –∑ –¥–æ–¥–∞–≤–∞–Ω–Ω—è–º timestamp
    """
    rows = []
    fetched_at = datetime.utcnow().isoformat() + "Z"
    
    for market in markets:
        if not isinstance(market, dict):
            continue
            
        # –û—Ç—Ä–∏–º—É—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∏–Ω–∫—É
        stats = market.get("marketStats", {})
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–ø–∏—Å –∑ —É—Å—ñ–º–∞ –¥–æ—Å—Ç—É–ø–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏
        row = {
            "timestamp": fetched_at,
            "fetched_at": fetched_at,
            "chain": chain,
            "market": market.get("name", "UNKNOWN"),
            
            # –¶—ñ–Ω–∏
            "lastPrice": float(stats.get("lastPrice", 0) or 0),
            "bidPrice": float(stats.get("bidPrice", 0) or 0),
            "askPrice": float(stats.get("askPrice", 0) or 0),
            "markPrice": float(stats.get("markPrice", 0) or 0),
            "indexPrice": float(stats.get("indexPrice", 0) or 0),
            
            # –û–±—Å—è–≥–∏ —Ç–∞ —ñ–Ω—Ç–µ—Ä–µ—Å–∏
            "dailyVolume": float(stats.get("dailyVolume", 0) or 0),
            "dailyVolumeBase": float(stats.get("dailyVolumeBase", 0) or 0),
            "openInterest": float(stats.get("openInterest", 0) or 0),
            
            # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
            "fundingRate": float(stats.get("fundingRate", 0) or 0),
            "priceChange24h": float(stats.get("priceChange24h", 0) or 0),
            
            # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ spread
            "spread_pct": 0,
        }
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ spread —è–∫—â–æ —î bid —Ç–∞ ask
        if row["bidPrice"] > 0 and row["askPrice"] > 0 and row["lastPrice"] > 0:
            row["spread_pct"] = (row["askPrice"] - row["bidPrice"]) / row["lastPrice"] * 100
            
        rows.append(row)
    
    return rows

def append_to_csv(df: pd.DataFrame, file_path: str, max_rows: int = 100000):
    """
    –£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –¥–æ CSV –∑ –æ–±–º–µ–∂–µ–Ω–Ω—è–º —Ä–æ–∑–º—ñ—Ä—É
    """
    if os.path.exists(file_path):
        # –ß–∏—Ç–∞—î–º–æ —ñ—Å–Ω—É—é—á—ñ –¥–∞–Ω—ñ
        try:
            existing = pd.read_csv(file_path)
            print(f"üìñ Found existing file with {len(existing)} rows")
            
            # –û–±'—î–¥–Ω—É—î–º–æ –∑ –Ω–æ–≤–∏–º–∏ –¥–∞–Ω–∏–º–∏
            combined = pd.concat([existing, df], ignore_index=True)
            
            # –°–æ—Ä—Ç—É—î–º–æ –ø–æ timestamp (–Ω–æ–≤—ñ—à—ñ –∑–∞–ø–∏—Å–∏ –∑–≤–µ—Ä—Ö—É)
            if 'timestamp' in combined.columns:
                combined = combined.sort_values('timestamp', ascending=False)
            elif 'fetched_at' in combined.columns:
                combined = combined.sort_values('fetched_at', ascending=False)
            
            # –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤
            combined = combined.head(max_rows)
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ
            combined.to_csv(file_path, index=False)
            print(f"‚úÖ Updated {os.path.basename(file_path)}: {len(combined)} total rows")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error reading existing file: {e}")
            df.to_csv(file_path, index=False)
            print(f"‚úÖ Created new {os.path.basename(file_path)}: {len(df)} rows")
    else:
        # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π —Ñ–∞–π–ª
        df.to_csv(file_path, index=False)
        print(f"‚úÖ Created {os.path.basename(file_path)}: {len(df)} rows")

def save_markets_data(df: pd.DataFrame):
    """–ó–±–µ—Ä—ñ–≥–∞—î –¥–∞–Ω—ñ —Ä–∏–Ω–∫—ñ–≤ –∑ —ñ—Å—Ç–æ—Ä—ñ—î—é"""
    file_path = os.path.join(UPLOADS_DIR, "extended_markets_data.csv")
    append_to_csv(df, file_path, max_rows=50000)

def save_trading_stats(stats_list: List[Dict]):
    """–ó–±–µ—Ä—ñ–≥–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ—Ä–≥—ñ–≤ –∑ —ñ—Å—Ç–æ—Ä—ñ—î—é"""
    if not stats_list:
        print("‚ö†Ô∏è No trading stats to save")
        return
    
    df = pd.DataFrame(stats_list)
    file_path = os.path.join(UPLOADS_DIR, "extended_trading_stats.csv")
    
    # –î–ª—è trading stats –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏ –ø–æ –¥–∞—Ç—ñ —Ç–∞ –º–µ—Ä–µ–∂—ñ
    if os.path.exists(file_path):
        try:
            existing = pd.read_csv(file_path)
            existing_keys = set(zip(existing['date'], existing['chain']))
            new_data = [stat for stat in stats_list 
                       if (stat['date'], stat['chain']) not in existing_keys]
            
            if new_data:
                new_df = pd.DataFrame(new_data)
                append_to_csv(new_df, file_path, max_rows=10000)
            else:
                print("‚ÑπÔ∏è No new trading stats to add")
        except Exception as e:
            print(f"‚ö†Ô∏è Error processing trading stats: {e}")
            append_to_csv(df, file_path, max_rows=10000)
    else:
        append_to_csv(df, file_path, max_rows=10000)

def save_tvl_data(tvl_data: Dict):
    """–ó–±–µ—Ä—ñ–≥–∞—î TVL –¥–∞–Ω—ñ –∑ —ñ—Å—Ç–æ—Ä—ñ—î—é —Ç–∞ –ø–æ–∫—Ä–∞—â–µ–Ω–æ—é –æ–±—Ä–æ–±–∫–æ—é"""
    if not tvl_data:
        print("‚ö†Ô∏è No TVL data to save")
        return
        
    current_time = datetime.utcnow().isoformat() + "Z"
    date_str = datetime.utcnow().strftime('%Y-%m-%d')
    
    tvl_records = []
    
    # –ó–∞–≥–∞–ª—å–Ω–∏–π TVL
    total_tvl = float(tvl_data.get('tvl', 0) or 0)
    tvl_records.append({
        'timestamp': current_time,
        'date': date_str,
        'chain': 'total',
        'tvl_usd': total_tvl
    })
    
    # TVL –ø–æ –æ–∫—Ä–µ–º–∏—Ö –º–µ—Ä–µ–∂–∞—Ö
    chain_tvls = tvl_data.get('chainTvls', {})
    for chain, tvl_value in chain_tvls.items():
        chain_clean = chain.lower().replace('-', '_')
        tvl_records.append({
            'timestamp': current_time,
            'date': date_str,
            'chain': chain_clean,
            'tvl_usd': float(tvl_value or 0)
        })
    
    # –î–æ–¥–∞—î–º–æ –æ–∫—Ä–µ–º–æ ethereum —Ç–∞ starknet —è–∫—â–æ —ó—Ö –Ω–µ–º–∞—î
    chains_found = set(rec['chain'] for rec in tvl_records)
    
    if 'ethereum' not in chains_found and 'Ethereum' in chain_tvls:
        tvl_records.append({
            'timestamp': current_time,
            'date': date_str, 
            'chain': 'ethereum',
            'tvl_usd': float(chain_tvls.get('Ethereum', 0) or 0)
        })
        
    if 'starknet' not in chains_found and 'Starknet' in chain_tvls:
        tvl_records.append({
            'timestamp': current_time,
            'date': date_str,
            'chain': 'starknet', 
            'tvl_usd': float(chain_tvls.get('Starknet', 0) or 0)
        })
    
    if tvl_records:
        df = pd.DataFrame(tvl_records)
        file_path = os.path.join(UPLOADS_DIR, "extended_tvl_data.csv")
        append_to_csv(df, file_path, max_rows=20000)

def save_onchain_metrics(metrics_list: List[Dict]):
    """–ó–±–µ—Ä—ñ–≥–∞—î on-chain –º–µ—Ç—Ä–∏–∫–∏"""
    if not metrics_list:
        return
        
    df = pd.DataFrame(metrics_list)
    file_path = os.path.join(UPLOADS_DIR, "extended_onchain_metrics.csv")
    append_to_csv(df, file_path, max_rows=10000)

def main():
    """
    –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è - –∑–±–∏—Ä–∞—î –≤—Å—ñ –¥–∞–Ω—ñ Extended –±—ñ—Ä–∂—ñ
    """
    print("üöÄ Starting Extended Exchange data collection...")
    print(f"üïí Current time: {datetime.utcnow().isoformat()}Z")
    ensure_uploads_dir()
    
    all_markets_data = []
    trading_stats = []
    onchain_metrics = []
    
    # === 1. –ó–ë–ò–†–ê–Ñ–ú–û –î–ê–ù–Ü –†–ò–ù–ö–Ü–í ===
    print("\n=== üìä COLLECTING MARKETS DATA ===")
    for chain in ['ethereum', 'starknet']:
        markets = fetch_markets_data(chain)
        if markets:
            normalized = normalize_markets_data(markets, chain)
            all_markets_data.extend(normalized)
            
            # –ó–±–∏—Ä–∞—î–º–æ also on-chain –º–µ—Ç—Ä–∏–∫–∏
            metrics = fetch_onchain_metrics(chain)
            if metrics:
                onchain_metrics.append(metrics)
    
    if all_markets_data:
        markets_df = pd.DataFrame(all_markets_data)
        save_markets_data(markets_df)
        print(f"üìä Processed {len(all_markets_data)} market records")
    
    # === 2. –ó–ë–ò–†–ê–Ñ–ú–û –°–¢–ê–¢–ò–°–¢–ò–ö–£ –¢–û–†–ì–Ü–í ===
    print("\n=== üìà COLLECTING TRADING STATISTICS ===")
    # –ó–±–∏—Ä–∞—î–º–æ –¥–∞–Ω—ñ –∑–∞ –æ—Å—Ç–∞–Ω–Ω—ñ 14 –¥–Ω—ñ–≤ –¥–ª—è –±—ñ–ª—å—à –ø–æ–≤–Ω–æ—ó –∫–∞—Ä—Ç–∏–Ω–∏
    for days_back in range(14):
        date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')
        
        for chain in ['ethereum', 'starknet']:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ü—è –º–µ—Ä–µ–∂–∞ –±—É–ª–∞ –∞–∫—Ç–∏–≤–Ω–∞ –Ω–∞ —Ç—É –¥–∞—Ç—É
            start_date = datetime.strptime(ENDPOINTS[chain]['start_date'], '%Y-%m-%d')
            check_date = datetime.strptime(date, '%Y-%m-%d')
            
            if check_date >= start_date:
                stats = fetch_trading_stats(chain, date)
                trading_stats.append(stats)
    
    if trading_stats:
        save_trading_stats(trading_stats)
        print(f"üìà Processed {len(trading_stats)} trading stat records")
    
    # === 3. –ó–ë–ò–†–ê–Ñ–ú–û TVL –î–ê–ù–Ü ===
    print("\n=== üí∞ COLLECTING TVL DATA ===")
    tvl_data = fetch_tvl_data()
    if tvl_data:
        save_tvl_data(tvl_data)
        print("üí∞ Processed TVL data")
    
    # === 4. –ó–ë–ï–†–Ü–ì–ê–Ñ–ú–û ON-CHAIN –ú–ï–¢–†–ò–ö–ò ===
    if onchain_metrics:
        save_onchain_metrics(onchain_metrics)
        print(f"‚õìÔ∏è Processed {len(onchain_metrics)} on-chain metric records")
    
    # === 5. SUMMARY ===
    print("\n=== ‚úÖ COLLECTION COMPLETED ===")
    print("üìÅ Check uploads/ directory for CSV files:")
    
    for filename in os.listdir(UPLOADS_DIR):
        if filename.endswith('.csv'):
            file_path = os.path.join(UPLOADS_DIR, filename)
            file_size = os.path.getsize(file_path)
            with open(file_path, 'r') as f:
                line_count = sum(1 for _ in f) - 1  # -1 –¥–ª—è header
            print(f"  üìÑ {filename}: {line_count} rows, {file_size} bytes")

if __name__ == "__main__":
    main()
