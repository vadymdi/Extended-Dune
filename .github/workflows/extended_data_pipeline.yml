name: Extended Exchange Data Pipeline
on:
  schedule:
    - cron: "0 */4 * * *"    # кожні 4 години для актуальних даних
  workflow_dispatch:          # можна запускати вручну
  push:
    branches: [ main ]
    paths: 
      - 'fetch_data_to_dune.py'  # перезапускати при зміні коду збору
      - 'scripts/upload_to_dune.py'

permissions:
  contents: write

jobs:
  collect-and-upload:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          persist-credentials: true
          token: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Collect Extended Exchange data
        run: |
          echo "🚀 Starting Extended data collection..."
          echo "📅 Collection time: $(date -u '+%Y-%m-%d %H:%M UTC')"
          python fetch_data_to_dune.py
          
      - name: Validate collected data
        run: |
          echo "🔍 Validating collected data..."
          python -c "
import os
import pandas as pd

uploads_dir = 'uploads'
if not os.path.exists(uploads_dir):
    print('❌ No uploads directory found')
    exit(1)

csv_files = [f for f in os.listdir(uploads_dir) if f.endswith('.csv')]
if not csv_files:
    print('❌ No CSV files found')
    exit(1)

print('📊 Data validation summary:')
print('=' * 50)

total_rows = 0
for filename in csv_files:
    filepath = os.path.join(uploads_dir, filename)
    try:
        df = pd.read_csv(filepath)
        rows = len(df)
        total_rows += rows
        size_mb = os.path.getsize(filepath) / (1024*1024)
        
        print(f'📄 {filename}:')
        print(f'   Rows: {rows:,}')
        print(f'   Size: {size_mb:.2f} MB')
        
        # Перевіряємо структуру даних
        if 'timestamp' in df.columns or 'fetched_at' in df.columns:
            time_col = 'timestamp' if 'timestamp' in df.columns else 'fetched_at'
            latest = df[time_col].max()
            oldest = df[time_col].min()
            print(f'   Time range: {oldest} to {latest}')
        
        print('   Columns:', ', '.join(df.columns[:5]) + ('...' if len(df.columns) > 5 else ''))
        print('-' * 50)
        
    except Exception as e:
        print(f'❌ Error validating {filename}: {e}')

print(f'✅ Total rows across all files: {total_rows:,}')
if total_rows == 0:
    print('❌ No data collected!')
    exit(1)
print('✅ Data validation passed')
"
          
      - name: Upload all data to Dune Analytics
        env:
          DUNE_API_KEY: ${{ secrets.DUNE_API_KEY }}
        run: |
          echo "⬆️ Uploading to Dune Analytics..."
          echo "🔑 API Key available: $([ -n '$DUNE_API_KEY' ] && echo 'YES' || echo 'NO')"
          
          # Показуємо що будемо загружати
          echo "📦 Files to upload:"
          ls -la uploads/*.csv 2>/dev/null || echo "No CSV files found"
          
          # Загружаємо
          python scripts/upload_to_dune.py
          
      - name: Commit updated data (optional)
        continue-on-error: true
        run: |
          git config user.name "extended-data-bot"
          git config user.email "bot@extended.exchange"
          git add uploads/
          
          # Перевіряємо чи є зміни
          if git diff --cached --quiet; then
            echo "ℹ️ No changes to commit"
          else
            # Створюємо informative commit message
            files_updated=$(git diff --cached --name-only | wc -l)
            commit_msg="📊 Update Extended data: ${files_updated} files $(date -u '+%Y-%m-%d %H:%M UTC')"
            
            echo "📝 Committing: $commit_msg"
            git commit -m "$commit_msg" || true
            
            # Push з retry mechanism
            for i in {1..3}; do
              if git push origin HEAD:main; then
                echo "✅ Successfully pushed changes"
                break
              else
                echo "⚠️ Push attempt $i failed, retrying in 5 seconds..."
                sleep 5
              fi
            done
          fi
          
      - name: Pipeline summary
        run: |
          echo "🎉 Extended Exchange data pipeline completed!"
          echo "=" * 60
          echo "📊 Data Summary:"
          
          if [ -d "uploads" ]; then
            for file in uploads/*.csv; do
              if [ -f "$file" ]; then
                filename=$(basename "$file")
                lines=$(wc -l < "$file" 2>/dev/null || echo "0")
                size=$(du -h "$file" 2>/dev/null | cut -f1 || echo "0")
                echo "  📄 $filename: $((lines-1)) data rows, $size"
              fi
            done
          fi
          
          echo ""
          echo "🔗 Check your Dune analytics dashboard:"
          echo "   - dune.vadymdi.dataset_extended_markets_data"
          echo "   - dune.vadymdi.dataset_extended_trading_stats"  
          echo "   - dune.vadymdi.dataset_extended_tvl_data"
          echo "   - dune.vadymdi.dataset_extended_onchain_metrics"
          echo ""
          echo "📅 Next automatic run: in 4 hours"
          echo "🔧 Manual run: Use 'Run workflow' button in GitHub Actions"
