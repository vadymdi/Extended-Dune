name: Extended Exchange Data Pipeline
on:
  schedule:
    - cron: "0 */4 * * *"    # ĞºĞ¾Ğ¶Ğ½Ñ– 4 Ğ³Ğ¾Ğ´Ğ¸Ğ½Ğ¸ Ğ´Ğ»Ñ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ¸Ñ…
  workflow_dispatch:          # Ğ¼Ğ¾Ğ¶Ğ½Ğ° Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ‚Ğ¸ Ğ²Ñ€ÑƒÑ‡Ğ½Ñƒ
  push:
    branches: [ main ]
    paths: 
      - 'fetch_data_to_dune.py'  # Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ¼Ñ–Ğ½Ñ– ĞºĞ¾Ğ´Ñƒ Ğ·Ğ±Ğ¾Ñ€Ñƒ
      - 'scripts/upload_to_dune.py'

permissions:
  contents: write

jobs:
  collect-and-upload:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          persist-credentials: true
          token: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Collect Extended Exchange data
        run: |
          echo "ğŸš€ Starting Extended data collection..."
          echo "ğŸ“… Collection time: $(date -u '+%Y-%m-%d %H:%M UTC')"
          python fetch_data_to_dune.py
          
      - name: Validate collected data
        run: |
          echo "ğŸ” Validating collected data..."
          python -c "
import os
import pandas as pd

uploads_dir = 'uploads'
if not os.path.exists(uploads_dir):
    print('âŒ No uploads directory found')
    exit(1)

csv_files = [f for f in os.listdir(uploads_dir) if f.endswith('.csv')]
if not csv_files:
    print('âŒ No CSV files found')
    exit(1)

print('ğŸ“Š Data validation summary:')
print('=' * 50)

total_rows = 0
for filename in csv_files:
    filepath = os.path.join(uploads_dir, filename)
    try:
        df = pd.read_csv(filepath)
        rows = len(df)
        total_rows += rows
        size_mb = os.path.getsize(filepath) / (1024*1024)
        
        print(f'ğŸ“„ {filename}:')
        print(f'   Rows: {rows:,}')
        print(f'   Size: {size_mb:.2f} MB')
        
        # ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ÑÑ”Ğ¼Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ°Ğ½Ğ¸Ñ…
        if 'timestamp' in df.columns or 'fetched_at' in df.columns:
            time_col = 'timestamp' if 'timestamp' in df.columns else 'fetched_at'
            latest = df[time_col].max()
            oldest = df[time_col].min()
            print(f'   Time range: {oldest} to {latest}')
        
        print('   Columns:', ', '.join(df.columns[:5]) + ('...' if len(df.columns) > 5 else ''))
        print('-' * 50)
        
    except Exception as e:
        print(f'âŒ Error validating {filename}: {e}')

print(f'âœ… Total rows across all files: {total_rows:,}')
if total_rows == 0:
    print('âŒ No data collected!')
    exit(1)
print('âœ… Data validation passed')
"
          
      - name: Upload all data to Dune Analytics
        env:
          DUNE_API_KEY: ${{ secrets.DUNE_API_KEY }}
        run: |
          echo "â¬†ï¸ Uploading to Dune Analytics..."
          echo "ğŸ”‘ API Key available: $([ -n '$DUNE_API_KEY' ] && echo 'YES' || echo 'NO')"
          
          # ĞŸĞ¾ĞºĞ°Ğ·ÑƒÑ”Ğ¼Ğ¾ Ñ‰Ğ¾ Ğ±ÑƒĞ´ĞµĞ¼Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ‚Ğ¸
          echo "ğŸ“¦ Files to upload:"
          ls -la uploads/*.csv 2>/dev/null || echo "No CSV files found"
          
          # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ”Ğ¼Ğ¾
          python scripts/upload_to_dune.py
          
      - name: Commit updated data (optional)
        continue-on-error: true
        run: |
          git config user.name "extended-data-bot"
          git config user.email "bot@extended.exchange"
          git add uploads/
          
          # ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ÑÑ”Ğ¼Ğ¾ Ñ‡Ğ¸ Ñ” Ğ·Ğ¼Ñ–Ğ½Ğ¸
          if git diff --cached --quiet; then
            echo "â„¹ï¸ No changes to commit"
          else
            # Ğ¡Ñ‚Ğ²Ğ¾Ñ€ÑÑ”Ğ¼Ğ¾ informative commit message
            files_updated=$(git diff --cached --name-only | wc -l)
            commit_msg="ğŸ“Š Update Extended data: ${files_updated} files $(date -u '+%Y-%m-%d %H:%M UTC')"
            
            echo "ğŸ“ Committing: $commit_msg"
            git commit -m "$commit_msg" || true
            
            # Push Ğ· retry mechanism
            for i in {1..3}; do
              if git push origin HEAD:main; then
                echo "âœ… Successfully pushed changes"
                break
              else
                echo "âš ï¸ Push attempt $i failed, retrying in 5 seconds..."
                sleep 5
              fi
            done
          fi
          
      - name: Pipeline summary
        run: |
          echo "ğŸ‰ Extended Exchange data pipeline completed!"
          echo "=" * 60
          echo "ğŸ“Š Data Summary:"
          
          if [ -d "uploads" ]; then
            for file in uploads/*.csv; do
              if [ -f "$file" ]; then
                filename=$(basename "$file")
                lines=$(wc -l < "$file" 2>/dev/null || echo "0")
                size=$(du -h "$file" 2>/dev/null | cut -f1 || echo "0")
                echo "  ğŸ“„ $filename: $((lines-1)) data rows, $size"
              fi
            done
          fi
          
          echo ""
          echo "ğŸ”— Check your Dune analytics dashboard:"
          echo "   - dune.vadymdi.dataset_extended_markets_data"
          echo "   - dune.vadymdi.dataset_extended_trading_stats"  
          echo "   - dune.vadymdi.dataset_extended_tvl_data"
          echo "   - dune.vadymdi.dataset_extended_onchain_metrics"
          echo ""
          echo "ğŸ“… Next automatic run: in 4 hours"
          echo "ğŸ”§ Manual run: Use 'Run workflow' button in GitHub Actions"
