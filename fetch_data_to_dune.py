# enhanced_fetch_data_to_dune.py
"""
–†–æ–∑—à–∏—Ä–µ–Ω–∏–π –∑–±—ñ—Ä –¥–∞–Ω–∏—Ö –∑ Extended API –¥–ª—è —É–Ω—ñ–∫–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –±—ñ—Ä–∂—ñ
–ó–±–∏—Ä–∞—î:
1. Markets data (—Ü—ñ–Ω–∏, –æ–±—Å—è–≥–∏, open interest) - –∑ —ñ—Å—Ç–æ—Ä—ñ—î—é
2. Trading statistics (—ñ—Å—Ç–æ—Ä–∏—á–Ω—ñ –æ–±—Å—è–≥–∏) - –Ω–∞–∫–æ–ø–∏—á—É–≤–∞–ª—å–Ω–æ
3. TVL –¥–∞–Ω—ñ –∑ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ñ–≤ (—á–µ—Ä–µ–∑ DeFiLlama API) - –∑ —á–∞—Å–æ–≤–∏–º–∏ –º—ñ—Ç–∫–∞–º–∏
4. Funding rates history - –Ω–æ–≤—ñ –¥–∞–Ω—ñ
5. Order book snapshots - –Ω–æ–≤—ñ –¥–∞–Ω—ñ
–ó–±–µ—Ä—ñ–≥–∞—î –≤—Å–µ –≤ –æ–∫—Ä–µ–º—ñ CSV —Ñ–∞–π–ª–∏ –¥–ª—è Dune Analytics –∑ –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—î—é
"""

import requests
import pandas as pd
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import json
import hashlib
import time

# === –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø ===
UPLOADS_DIR = "uploads"
TIMEOUT = 30
MAX_RETRIES = 3
RETRY_DELAY = 2

# API endpoints –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –º–µ—Ä–µ–∂
ENDPOINTS = {
    'ethereum': {
        'markets': 'https://api.extended.exchange/api/v1/info/markets',
        'trading': 'https://api.extended.exchange/api/v1/exchange/stats/trading',
        'orderbook': 'https://api.extended.exchange/api/v1/orderbook',
        'funding': 'https://api.extended.exchange/api/v1/funding',
        'start_date': '2025-03-11'
    },
    'starknet': {
        'markets': 'https://api.starknet.extended.exchange/api/v1/info/markets',
        'trading': 'https://api.starknet.extended.exchange/api/v1/exchange/stats/trading',
        'orderbook': 'https://api.starknet.extended.exchange/api/v1/orderbook',
        'funding': 'https://api.starknet.extended.exchange/api/v1/funding',
        'start_date': '2025-08-10'
    }
}

# DeFiLlama TVL endpoint
DEFILLAMA_TVL_API = "https://api.llama.fi/protocol/extended"

def ensure_uploads_dir():
    """–°—Ç–≤–æ—Ä—é—î –ø–∞–ø–∫—É uploads —è–∫—â–æ —ó—ó –Ω–µ–º–∞—î"""
    os.makedirs(UPLOADS_DIR, exist_ok=True)
    print("üìÅ Uploads directory ready")

def make_request_with_retry(url: str, params: Dict = None) -> Optional[Dict]:
    """–†–æ–±–∏—Ç—å –∑–∞–ø–∏—Ç –∑ –ø–æ–≤—Ç–æ—Ä–æ–º –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ"""
    for attempt in range(MAX_RETRIES):
        try:
            response = requests.get(url, params=params, timeout=TIMEOUT)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"‚ùå Attempt {attempt + 1}/{MAX_RETRIES} failed for {url}: {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)
            else:
                return None
    return None

def fetch_markets_data(chain: str) -> List[Dict]:
    """
    –û—Ç—Ä–∏–º—É—î –¥–∞–Ω—ñ —Ä–∏–Ω–∫—ñ–≤ (markets) –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó –º–µ—Ä–µ–∂—ñ
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ —Ä–∏–Ω–∫—ñ–≤ –∑ —Ü—ñ–Ω–∞–º–∏, –æ–±—Å—è–≥–∞–º–∏ —Ç–∞ open interest
    """
    url = ENDPOINTS[chain]['markets']
    print(f"üîÑ Fetching markets data for {chain}...")
    
    data = make_request_with_retry(url)
    if not data:
        return []
    
    markets = data.get("data", []) if isinstance(data, dict) else data
    print(f"‚úÖ Got {len(markets)} markets from {chain}")
    return markets or []

def fetch_trading_stats(chain: str, date: str) -> Dict:
    """
    –û—Ç—Ä–∏–º—É—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ—Ä–≥—ñ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó –¥–∞—Ç–∏ —Ç–∞ –º–µ—Ä–µ–∂—ñ
    """
    url = ENDPOINTS[chain]['trading']
    params = {'fromDate': date, 'toDate': date}
    print(f"üîÑ Fetching trading stats for {chain} on {date}...")
    
    data = make_request_with_retry(url, params)
    if not data:
        return {'date': date, 'chain': chain, 'daily_volume': 0, 'trades_count': 0}
    
    # –û–±—Ä–æ–±–ª—è—î–º–æ –¥–∞–Ω—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    if isinstance(data, dict) and 'data' in data:
        trading_data = data['data']
        if trading_data:
            total_volume = sum(float(item.get('tradingVolume', 0)) for item in trading_data)
            unique_traders = len(set(item.get('trader', '') for item in trading_data if item.get('trader')))
            
            return {
                'date': date,
                'chain': chain,
                'daily_volume': total_volume,
                'trades_count': len(trading_data),
                'unique_traders': unique_traders,
                'avg_trade_size': total_volume / len(trading_data) if trading_data else 0
            }
    
    return {'date': date, 'chain': chain, 'daily_volume': 0, 'trades_count': 0, 'unique_traders': 0, 'avg_trade_size': 0}

def fetch_funding_rates(chain: str) -> List[Dict]:
    """
    –û—Ç—Ä–∏–º—É—î —ñ—Å—Ç–æ—Ä—ñ—é funding rates –¥–ª—è –≤—Å—ñ—Ö —Ä–∏–Ω–∫—ñ–≤
    """
    url = ENDPOINTS[chain]['funding']
    print(f"üîÑ Fetching funding rates for {chain}...")
    
    data = make_request_with_retry(url)
    if not data:
        return []
    
    funding_data = data.get("data", []) if isinstance(data, dict) else data
    
    # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –¥–∞–Ω—ñ
    normalized_data = []
    for item in funding_data:
        if isinstance(item, dict):
            normalized_data.append({
                'chain': chain,
                'market': item.get('market', 'UNKNOWN'),
                'funding_rate': float(item.get('fundingRate', 0) or 0),
                'funding_time': item.get('fundingTime', ''),
                'next_funding_time': item.get('nextFundingTime', ''),
                'fetched_at': datetime.utcnow().isoformat() + "Z"
            })
    
    print(f"‚úÖ Got {len(normalized_data)} funding records from {chain}")
    return normalized_data

def fetch_orderbook_snapshots(chain: str, markets: List[str]) -> List[Dict]:
    """
    –û—Ç—Ä–∏–º—É—î snapshots order book –¥–ª—è —Ç–æ–ø —Ä–∏–Ω–∫—ñ–≤
    """
    snapshots = []
    url = ENDPOINTS[chain]['orderbook']
    
    # –ë–µ—Ä–µ–º–æ —Ç—ñ–ª—å–∫–∏ —Ç–æ–ø 5 —Ä–∏–Ω–∫—ñ–≤ —â–æ–± –Ω–µ –ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂—É–≤–∞—Ç–∏ API
    top_markets = markets[:5]
    
    for market in top_markets:
        print(f"üîÑ Fetching orderbook for {chain}:{market}...")
        params = {'market': market}
        
        data = make_request_with_retry(url, params)
        if not data:
            continue
            
        orderbook = data.get("data", {})
        bids = orderbook.get("bids", [])
        asks = orderbook.get("asks", [])
        
        if bids or asks:
            # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –º–µ—Ç—Ä–∏–∫–∏ –≥–ª–∏–±–∏–Ω–∏ —Ä–∏–Ω–∫—É
            bid_depth = sum(float(bid[1]) for bid in bids[:10])  # –¢–æ–ø 10 –±—ñ–¥—ñ–≤
            ask_depth = sum(float(ask[1]) for ask in asks[:10])  # –¢–æ–ø 10 –∞—Å–∫—ñ–≤
            
            best_bid = float(bids[0][0]) if bids else 0
            best_ask = float(asks[0][0]) if asks else 0
            spread = best_ask - best_bid if best_bid and best_ask else 0
            
            snapshots.append({
                'chain': chain,
                'market': market,
                'best_bid': best_bid,
                'best_ask': best_ask,
                'spread': spread,
                'spread_pct': (spread / best_bid * 100) if best_bid else 0,
                'bid_depth_10': bid_depth,
                'ask_depth_10': ask_depth,
                'total_bids': len(bids),
                'total_asks': len(asks),
                'fetched_at': datetime.utcnow().isoformat() + "Z"
            })
    
    print(f"‚úÖ Got {len(snapshots)} orderbook snapshots from {chain}")
    return snapshots

def fetch_tvl_data() -> Optional[Dict]:
    """–û—Ç—Ä–∏–º—É—î TVL –¥–∞–Ω—ñ –∑ DeFiLlama"""
    print("üîÑ Fetching TVL data from DeFiLlama...")
    
    data = make_request_with_retry(DEFILLAMA_TVL_API)
    if data:
        print("‚úÖ Got TVL data from DeFiLlama")
    return data

def normalize_markets_data(markets: List[Dict], chain: str) -> List[Dict]:
    """–ù–æ—Ä–º–∞–ª—ñ–∑—É—î –¥–∞–Ω—ñ —Ä–∏–Ω–∫—ñ–≤ –≤ —É–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç"""
    rows = []
    fetched_at = datetime.utcnow().isoformat() + "Z"
    
    for market in markets:
        if not isinstance(market, dict):
            continue
            
        stats = market.get("marketStats", {})
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π ID –¥–ª—è –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—ó
        unique_id = f"{chain}_{market.get('name', 'UNKNOWN')}_{fetched_at[:16]}"  # –î–æ —Ö–≤–∏–ª–∏–Ω–∏
        
        row = {
            "unique_id": unique_id,
            "fetched_at": fetched_at,
            "chain": chain,
            "market": market.get("name", "UNKNOWN"),
            
            # –¶—ñ–Ω–∏
            "lastPrice": float(stats.get("lastPrice", 0) or 0),
            "bidPrice": float(stats.get("bidPrice", 0) or 0),
            "askPrice": float(stats.get("askPrice", 0) or 0),
            "markPrice": float(stats.get("markPrice", 0) or 0),
            "indexPrice": float(stats.get("indexPrice", 0) or 0),
            
            # –û–±—Å—è–≥–∏ —Ç–∞ —ñ–Ω—Ç–µ—Ä–µ—Å–∏
            "dailyVolume": float(stats.get("dailyVolume", 0) or 0),
            "dailyVolumeBase": float(stats.get("dailyVolumeBase", 0) or 0),
            "openInterest": float(stats.get("openInterest", 0) or 0),
            
            # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
            "fundingRate": float(stats.get("fundingRate", 0) or 0),
            "priceChange24h": float(stats.get("priceChange24h", 0) or 0),
            
            # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ spread
            "spread_pct": 0,
        }
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ spread
        if row["bidPrice"] > 0 and row["askPrice"] > 0 and row["lastPrice"] > 0:
            row["spread_pct"] = (row["askPrice"] - row["bidPrice"]) / row["lastPrice"] * 100
            
        rows.append(row)
    
    return rows

def save_data_with_deduplication(df: pd.DataFrame, filename: str, unique_columns: List[str]):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –¥–∞–Ω—ñ –∑ –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—î—é —Ç–∞ –æ–±–º–µ–∂–µ–Ω–Ω—è–º —Ä–æ–∑–º—ñ—Ä—É
    """
    file_path = os.path.join(UPLOADS_DIR, filename)
    
    if os.path.exists(file_path):
        try:
            # –ß–∏—Ç–∞—î–º–æ —ñ—Å–Ω—É—é—á—ñ –¥–∞–Ω—ñ
            existing = pd.read_csv(file_path)
            
            # –û–±'—î–¥–Ω—É—î–º–æ –∑ –Ω–æ–≤–∏–º–∏
            combined = pd.concat([existing, df], ignore_index=True)
            
            # –î–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—è –ø–æ —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
            combined = combined.drop_duplicates(subset=unique_columns, keep='last')
            
            # –°–æ—Ä—Ç—É—î–º–æ –ø–æ —á–∞—Å—É (–Ω–æ–≤—ñ—à—ñ –∑–∞–ø–∏—Å–∏ –∑–≤–µ—Ä—Ö—É)
            if 'fetched_at' in combined.columns:
                combined = combined.sort_values('fetched_at', ascending=False)
            elif 'date' in combined.columns:
                combined = combined.sort_values('date', ascending=False)
            
            # –û–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—É
            max_rows = 100000 if 'markets' in filename else 50000
            combined = combined.head(max_rows)
            
            combined.to_csv(file_path, index=False)
            print(f"‚úÖ Updated {filename}: {len(df)} new rows, {len(combined)} total rows")
            
        except Exception as e:
            print(f"‚ùå Error updating {filename}: {e}")
            df.to_csv(file_path, index=False)
            print(f"‚úÖ Recreated {filename}: {len(df)} rows")
    else:
        # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π —Ñ–∞–π–ª
        df.to_csv(file_path, index=False)
        print(f"‚úÖ Created {filename}: {len(df)} rows")

def save_tvl_data(tvl_data: Dict):
    """–ó–±–µ—Ä—ñ–≥–∞—î TVL –¥–∞–Ω—ñ –∑ —á–∞—Å–æ–≤–æ—é –º—ñ—Ç–∫–æ—é - –ø—Ä–æ—Å—Ç–∏–π —Ñ–æ—Ä–º–∞—Ç"""
    if not tvl_data:
        print("‚ö†Ô∏è No TVL data to save")
        return
        
    try:
        current_time = datetime.utcnow().isoformat() + "Z"
        current_date = current_time[:10]  # YYYY-MM-DD
        tvl_records = []
        
        # –ó–∞–≥–∞–ª—å–Ω–∏–π –ø–æ—Ç–æ—á–Ω–∏–π TVL
        total_tvl = tvl_data.get('tvl', 0)
        if total_tvl > 0:
            tvl_records.append({
                'fetched_at': current_time,
                'date': current_date,
                'chain': 'total',
                'tvl_usd': float(total_tvl)
            })
        
        # TVL –ø–æ –æ–∫—Ä–µ–º–∏—Ö –º–µ—Ä–µ–∂–∞—Ö (–ø–æ—Ç–æ—á–Ω–∏–π)
        chain_tvls = tvl_data.get('chainTvls', {})
        for chain, tvl_value in chain_tvls.items():
            if isinstance(tvl_value, (int, float)) and tvl_value > 0:
                tvl_records.append({
                    'fetched_at': current_time,
                    'date': current_date,
                    'chain': str(chain).lower(),
                    'tvl_usd': float(tvl_value)
                })
        
        if tvl_records:
            df = pd.DataFrame(tvl_records)
            save_data_with_deduplication(df, "extended_tvl_data.csv", ['date', 'chain'])
        else:
            print("‚ö†Ô∏è No valid TVL data to save")
            
    except Exception as e:
        print(f"‚ùå Error processing TVL data: {e}")
        print("TVL data structure:", tvl_data)

def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è - –∑–±–∏—Ä–∞—î –≤—Å—ñ –¥–∞–Ω—ñ Extended –±—ñ—Ä–∂—ñ"""
    print("üöÄ Starting Enhanced Extended Exchange data collection...")
    ensure_uploads_dir()
    
    # === 1. –ó–ë–ò–†–ê–Ñ–ú–û –î–ê–ù–Ü –†–ò–ù–ö–Ü–í ===
    all_markets_data = []
    all_funding_data = []
    all_orderbook_data = []
    
    for chain in ['ethereum', 'starknet']:
        # Markets
        markets = fetch_markets_data(chain)
        if markets:
            normalized = normalize_markets_data(markets, chain)
            all_markets_data.extend(normalized)
            
            # Funding rates
            funding_data = fetch_funding_rates(chain)
            all_funding_data.extend(funding_data)
            
            # Order book –¥–ª—è —Ç–æ–ø —Ä–∏–Ω–∫—ñ–≤
            market_names = [m.get('name', '') for m in markets if m.get('name')]
            orderbook_data = fetch_orderbook_snapshots(chain, market_names)
            all_orderbook_data.extend(orderbook_data)
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ markets data
    if all_markets_data:
        markets_df = pd.DataFrame(all_markets_data)
        save_data_with_deduplication(markets_df, "extended_markets_data.csv", ['unique_id'])
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ funding data
    if all_funding_data:
        funding_df = pd.DataFrame(all_funding_data)
        save_data_with_deduplication(funding_df, "extended_funding_rates.csv", ['chain', 'market', 'funding_time'])
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ orderbook data
    if all_orderbook_data:
        orderbook_df = pd.DataFrame(all_orderbook_data)
        save_data_with_deduplication(orderbook_df, "extended_orderbook_snapshots.csv", ['chain', 'market', 'fetched_at'])
    
    # === 2. –ó–ë–ò–†–ê–Ñ–ú–û –°–¢–ê–¢–ò–°–¢–ò–ö–£ –¢–û–†–ì–Ü–í ===
    trading_stats = []
    
    # –ó–±–∏—Ä–∞—î–º–æ –¥–∞–Ω—ñ –∑–∞ –æ—Å—Ç–∞–Ω–Ω—ñ 30 –¥–Ω—ñ–≤ (–±—ñ–ª—å—à–µ —ñ—Å—Ç–æ—Ä—ñ—ó)
    for days_back in range(30):
        date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')
        
        for chain in ['ethereum', 'starknet']:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –º–µ—Ä–µ–∂—ñ
            start_date = datetime.strptime(ENDPOINTS[chain]['start_date'], '%Y-%m-%d')
            check_date = datetime.strptime(date, '%Y-%m-%d')
            
            if check_date >= start_date:
                stats = fetch_trading_stats(chain, date)
                trading_stats.append(stats)
    
    if trading_stats:
        trading_df = pd.DataFrame(trading_stats)
        save_data_with_deduplication(trading_df, "extended_trading_stats.csv", ['date', 'chain'])
    
    # === 3. –ó–ë–ò–†–ê–Ñ–ú–û TVL –î–ê–ù–Ü ===
    tvl_data = fetch_tvl_data()
    if tvl_data:
        save_tvl_data(tvl_data)
    
    print("‚úÖ Enhanced data collection completed!")
    print("üìÅ Check uploads/ directory for CSV files:")
    print("   - extended_markets_data.csv (markets with prices, volumes, OI)")
    print("   - extended_trading_stats.csv (daily trading statistics)")
    print("   - extended_tvl_data.csv (TVL data)")
    print("   - extended_funding_rates.csv (funding rates history)")
    print("   - extended_orderbook_snapshots.csv (order book depth)")

if __name__ == "__main__":
    main()
