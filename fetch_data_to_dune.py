# fetch_data_to_dune.py
"""
–ê–¥–∞–ø—Ç–æ–≤–∞–Ω–æ –∑ fetch_volume_fees.py –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ Dune Analytics
–ó–±–µ—Ä—ñ–≥–∞—î –¥–∞–Ω—ñ –≤ /uploads/ –ø–∞–ø–∫—É –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤ Dune
"""
import requests
import pandas as pd
import os
from datetime import datetime

API_URL = "https://api.starknet.extended.exchange/api/v1/info/markets"
UPLOADS_DIR = "uploads"
OUT_FILE = os.path.join(UPLOADS_DIR, "extended_markets_data.csv")
TIMEOUT = 20

def fetch_markets():
    """–û—Ç—Ä–∏–º—É—î –¥–∞–Ω—ñ –∑ Extended API"""
    try:
        r = requests.get(API_URL, timeout=TIMEOUT)
        r.raise_for_status()
        payload = r.json()
        markets = payload.get("data") if isinstance(payload, dict) else payload
        return markets or []
    except Exception as e:
        print("‚ùå API Error:", e)
        return []

def normalize_markets(markets):
    """–ù–æ—Ä–º–∞–ª—ñ–∑—É—î –¥–∞–Ω—ñ –≤ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç"""
    rows = []
    fetched_at = datetime.utcnow().isoformat() + "Z"
    
    for m in markets:
        stats = m.get("marketStats") if isinstance(m, dict) else {}
        rows.append({
            "fetched_at": fetched_at,
            "market": m.get("name") if isinstance(m, dict) else str(m),
            "dailyVolume": stats.get("dailyVolume") or 0,
            "dailyVolumeBase": stats.get("dailyVolumeBase") or 0,
            "openInterest": stats.get("openInterest") or stats.get("openInterestBase") or 0,
            "fundingRate": stats.get("fundingRate") or 0,
            "lastPrice": stats.get("lastPrice") or 0,
            "bidPrice": stats.get("bidPrice") or 0,
            "askPrice": stats.get("askPrice") or 0,
            "markPrice": stats.get("markPrice") or 0,
            "indexPrice": stats.get("indexPrice") or 0
        })
    return rows

def save_for_dune(new_df):
    """–ó–±–µ—Ä—ñ–≥–∞—î –¥–∞–Ω—ñ –≤ uploads/ –ø–∞–ø–∫—É –¥–ª—è Dune"""
    os.makedirs(UPLOADS_DIR, exist_ok=True)
    
    # –î–ª—è Dune –∫—Ä–∞—â–µ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ –≤—Å—ñ –¥–∞–Ω—ñ –∑ timestamp
    # Dune —Å–∞–º –∫–µ—Ä—É–≤–∞—Ç–∏–º–µ –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—î—é —á–µ—Ä–µ–∑ SQL queries
    if os.path.exists(OUT_FILE):
        # –ß–∏—Ç–∞—î–º–æ —ñ—Å–Ω—É—é—á—ñ –¥–∞–Ω—ñ
        existing = pd.read_csv(OUT_FILE)
        # –î–æ–¥–∞—î–º–æ –Ω–æ–≤—ñ –¥–∞–Ω—ñ
        combined = pd.concat([existing, new_df], ignore_index=True)
        
        # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ —á–∞—Å–æ–º (–Ω–∞–π–Ω–æ–≤—ñ—à—ñ –∑–≤–µ—Ä—Ö—É)
        combined['fetched_at_sort'] = pd.to_datetime(combined['fetched_at'], errors='coerce')
        combined = combined.sort_values('fetched_at_sort', ascending=False)
        combined = combined.drop(columns=['fetched_at_sort'])
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –æ—Å—Ç–∞–Ω–Ω—ñ 10000 –∑–∞–ø–∏—Å—ñ–≤ —â–æ–± –Ω–µ –ø–µ—Ä–µ–≤–∏—â–∏—Ç–∏ –ª—ñ–º—ñ—Ç Dune
        combined = combined.head(10000)
        combined.to_csv(OUT_FILE, index=False)
        print(f"‚úÖ Updated CSV with {len(combined)} total rows")
    else:
        # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π —Ñ–∞–π–ª
        new_df.to_csv(OUT_FILE, index=False)
        print(f"‚úÖ Created new CSV with {len(new_df)} rows")

def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    print("üöÄ Fetching Extended markets data...")
    
    markets = fetch_markets()
    rows = normalize_markets(markets)
    
    if not rows:
        print("‚ùå No data received from API")
        return
    
    df = pd.DataFrame(rows)
    save_for_dune(df)
    print(f"üìä Processed {len(rows)} markets")

if __name__ == "__main__":
    main()
