# enhanced_fetch_data_to_dune.py
"""
–†–æ–∑—à–∏—Ä–µ–Ω–∏–π –∑–±—ñ—Ä –¥–∞–Ω–∏—Ö –∑ Extended API –¥–ª—è —É–Ω—ñ–∫–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –±—ñ—Ä–∂—ñ
–ó–±–∏—Ä–∞—î:
1. Markets data (—Ü—ñ–Ω–∏, –æ–±—Å—è–≥–∏, open interest)
2. Trading statistics (—ñ—Å—Ç–æ—Ä–∏—á–Ω—ñ –æ–±—Å—è–≥–∏)
3. TVL –¥–∞–Ω—ñ –∑ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ñ–≤ (—á–µ—Ä–µ–∑ DeFiLlama API)
–ó–±–µ—Ä—ñ–≥–∞—î –≤—Å–µ –≤ –æ–∫—Ä–µ–º—ñ CSV —Ñ–∞–π–ª–∏ –¥–ª—è Dune Analytics
"""

import requests
import pandas as pd
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import json

# === –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø ===
UPLOADS_DIR = "uploads"
TIMEOUT = 30

# API endpoints –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –º–µ—Ä–µ–∂
ENDPOINTS = {
    'ethereum': {
        'markets': 'https://api.extended.exchange/api/v1/info/markets',
        'trading': 'https://api.extended.exchange/api/v1/exchange/stats/trading',
        'start_date': '2025-03-11'
    },
    'starknet': {
        'markets': 'https://api.starknet.extended.exchange/api/v1/info/markets',
        'trading': 'https://api.starknet.extended.exchange/api/v1/exchange/stats/trading',
        'start_date': '2025-08-10'
    }
}

# DeFiLlama TVL endpoint
DEFILLAMA_TVL_API = "https://api.llama.fi/protocol/extended"

def ensure_uploads_dir():
    """–°—Ç–≤–æ—Ä—é—î –ø–∞–ø–∫—É uploads —è–∫—â–æ —ó—ó –Ω–µ–º–∞—î"""
    os.makedirs(UPLOADS_DIR, exist_ok=True)
    print("üìÅ Uploads directory ready")

def fetch_markets_data(chain: str) -> List[Dict]:
    """
    –û—Ç—Ä–∏–º—É—î –¥–∞–Ω—ñ —Ä–∏–Ω–∫—ñ–≤ (markets) –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó –º–µ—Ä–µ–∂—ñ
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ —Ä–∏–Ω–∫—ñ–≤ –∑ —Ü—ñ–Ω–∞–º–∏, –æ–±—Å—è–≥–∞–º–∏ —Ç–∞ open interest
    """
    try:
        url = ENDPOINTS[chain]['markets']
        print(f"üîÑ Fetching markets data for {chain}...")
        
        response = requests.get(url, timeout=TIMEOUT)
        response.raise_for_status()
        
        data = response.json()
        markets = data.get("data", []) if isinstance(data, dict) else data
        
        print(f"‚úÖ Got {len(markets)} markets from {chain}")
        return markets or []
        
    except Exception as e:
        print(f"‚ùå Error fetching {chain} markets: {e}")
        return []

def fetch_trading_stats(chain: str, date: str) -> Dict:
    """
    –û—Ç—Ä–∏–º—É—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ—Ä–≥—ñ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó –¥–∞—Ç–∏ —Ç–∞ –º–µ—Ä–µ–∂—ñ
    date —Ñ–æ—Ä–º–∞—Ç: YYYY-MM-DD
    """
    try:
        url = f"{ENDPOINTS[chain]['trading']}?fromDate={date}&toDate={date}"
        print(f"üîÑ Fetching trading stats for {chain} on {date}...")
        
        response = requests.get(url, timeout=TIMEOUT)
        response.raise_for_status()
        
        data = response.json()
        
        # –û–±—Ä–æ–±–ª—è—î–º–æ –¥–∞–Ω—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if isinstance(data, dict) and 'data' in data:
            trading_data = data['data']
            if trading_data:
                total_volume = sum(float(item.get('tradingVolume', 0)) for item in trading_data)
                return {
                    'date': date,
                    'chain': chain,
                    'daily_volume': total_volume,
                    'trades_count': len(trading_data)
                }
        
        return {'date': date, 'chain': chain, 'daily_volume': 0, 'trades_count': 0}
        
    except Exception as e:
        print(f"‚ùå Error fetching trading stats for {chain}: {e}")
        return {'date': date, 'chain': chain, 'daily_volume': 0, 'trades_count': 0}

def fetch_tvl_data() -> Optional[Dict]:
    """
    –û—Ç—Ä–∏–º—É—î TVL –¥–∞–Ω—ñ –∑ DeFiLlama
    """
    try:
        print("üîÑ Fetching TVL data from DeFiLlama...")
        
        response = requests.get(DEFILLAMA_TVL_API, timeout=TIMEOUT)
        response.raise_for_status()
        
        data = response.json()
        print("‚úÖ Got TVL data from DeFiLlama")
        return data
        
    except Exception as e:
        print(f"‚ùå Error fetching TVL data: {e}")
        return None

def normalize_markets_data(markets: List[Dict], chain: str) -> List[Dict]:
    """
    –ù–æ—Ä–º–∞–ª—ñ–∑—É—î –¥–∞–Ω—ñ —Ä–∏–Ω–∫—ñ–≤ –≤ —É–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç
    """
    rows = []
    fetched_at = datetime.utcnow().isoformat() + "Z"
    
    for market in markets:
        if not isinstance(market, dict):
            continue
            
        # –û—Ç—Ä–∏–º—É—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∏–Ω–∫—É
        stats = market.get("marketStats", {})
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–ø–∏—Å –∑ —É—Å—ñ–º–∞ –¥–æ—Å—Ç—É–ø–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏
        row = {
            "fetched_at": fetched_at,
            "chain": chain,
            "market": market.get("name", "UNKNOWN"),
            
            # –¶—ñ–Ω–∏
            "lastPrice": float(stats.get("lastPrice", 0) or 0),
            "bidPrice": float(stats.get("bidPrice", 0) or 0),
            "askPrice": float(stats.get("askPrice", 0) or 0),
            "markPrice": float(stats.get("markPrice", 0) or 0),
            "indexPrice": float(stats.get("indexPrice", 0) or 0),
            
            # –û–±—Å—è–≥–∏ —Ç–∞ —ñ–Ω—Ç–µ—Ä–µ—Å–∏
            "dailyVolume": float(stats.get("dailyVolume", 0) or 0),
            "dailyVolumeBase": float(stats.get("dailyVolumeBase", 0) or 0),
            "openInterest": float(stats.get("openInterest", 0) or 0),
            
            # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
            "fundingRate": float(stats.get("fundingRate", 0) or 0),
            "priceChange24h": float(stats.get("priceChange24h", 0) or 0),
            
            # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ spread
            "spread_pct": 0,
        }
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ spread —è–∫—â–æ —î bid —Ç–∞ ask
        if row["bidPrice"] > 0 and row["askPrice"] > 0 and row["lastPrice"] > 0:
            row["spread_pct"] = (row["askPrice"] - row["bidPrice"]) / row["lastPrice"] * 100
            
        rows.append(row)
    
    return rows

def save_markets_data(df: pd.DataFrame):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –¥–∞–Ω—ñ —Ä–∏–Ω–∫—ñ–≤ –∑ —ñ—Å—Ç–æ—Ä—ñ—î—é (–¥–æ–¥–∞—î –Ω–æ–≤—ñ –∑–∞–ø–∏—Å–∏)
    """
    file_path = os.path.join(UPLOADS_DIR, "extended_markets_data.csv")
    
    if os.path.exists(file_path):
        # –ß–∏—Ç–∞—î–º–æ —ñ—Å–Ω—É—é—á—ñ –¥–∞–Ω—ñ
        existing = pd.read_csv(file_path)
        
        # –û–±'—î–¥–Ω—É—î–º–æ –∑ –Ω–æ–≤–∏–º–∏
        combined = pd.concat([existing, df], ignore_index=True)
        
        # –°–æ—Ä—Ç—É—î–º–æ –ø–æ —á–∞—Å—É (–Ω–æ–≤—ñ—à—ñ –∑–∞–ø–∏—Å–∏ –∑–≤–µ—Ä—Ö—É)
        combined = combined.sort_values('fetched_at', ascending=False)
        
        # –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤ (–æ—Å—Ç–∞–Ω–Ω—ñ 50,000 —â–æ–± –Ω–µ –ø–µ—Ä–µ–≤–∏—â–∏—Ç–∏ –ª—ñ–º—ñ—Ç Dune)
        combined = combined.head(50000)
        
        combined.to_csv(file_path, index=False)
        print(f"‚úÖ Updated markets data: {len(combined)} total rows")
    else:
        # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π —Ñ–∞–π–ª
        df.to_csv(file_path, index=False)
        print(f"‚úÖ Created markets data file: {len(df)} rows")

def save_trading_stats(stats_list: List[Dict]):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ—Ä–≥—ñ–≤
    """
    if not stats_list:
        print("‚ö†Ô∏è No trading stats to save")
        return
        
    df = pd.DataFrame(stats_list)
    file_path = os.path.join(UPLOADS_DIR, "extended_trading_stats.csv")
    
    if os.path.exists(file_path):
        existing = pd.read_csv(file_path)
        
        # –£–Ω–∏–∫–∞—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ –ø–æ –¥–∞—Ç—ñ —Ç–∞ –º–µ—Ä–µ–∂—ñ
        existing_keys = set(zip(existing['date'], existing['chain']))
        new_data = [stat for stat in stats_list 
                   if (stat['date'], stat['chain']) not in existing_keys]
        
        if new_data:
            new_df = pd.DataFrame(new_data)
            combined = pd.concat([existing, new_df], ignore_index=True)
            combined = combined.sort_values('date', ascending=False)
            combined.to_csv(file_path, index=False)
            print(f"‚úÖ Updated trading stats: {len(new_data)} new rows")
        else:
            print("‚ÑπÔ∏è No new trading stats to add")
    else:
        df.to_csv(file_path, index=False)
        print(f"‚úÖ Created trading stats file: {len(df)} rows")

def save_tvl_data(tvl_data: Dict):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î TVL –¥–∞–Ω—ñ –∑ —á–∞—Å–æ–≤–æ—é –º—ñ—Ç–∫–æ—é
    """
    if not tvl_data:
        print("‚ö†Ô∏è No TVL data to save")
        return
        
    # –î–æ–¥–∞—î–º–æ —á–∞—Å–æ–≤—É –º—ñ—Ç–∫—É
    current_time = datetime.utcnow().isoformat() + "Z"
    
    # –í–∏—Ç—è–≥—É—î–º–æ TVL –ø–æ –º–µ—Ä–µ–∂–∞—Ö
    tvl_records = []
    
    # –ó–∞–≥–∞–ª—å–Ω–∏–π TVL
    total_tvl = tvl_data.get('tvl', 0)
    tvl_records.append({
        'fetched_at': current_time,
        'chain': 'total',
        'tvl_usd': total_tvl
    })
    
    # TVL –ø–æ –æ–∫—Ä–µ–º–∏—Ö –º–µ—Ä–µ–∂–∞—Ö
    chain_tvls = tvl_data.get('chainTvls', {})
    for chain, tvl_value in chain_tvls.items():
        tvl_records.append({
            'fetched_at': current_time,
            'chain': chain.lower(),
            'tvl_usd': tvl_value
        })
    
    if tvl_records:
        df = pd.DataFrame(tvl_records)
        file_path = os.path.join(UPLOADS_DIR, "extended_tvl_data.csv")
        
        if os.path.exists(file_path):
            existing = pd.read_csv(file_path)
            combined = pd.concat([existing, df], ignore_index=True)
            combined = combined.sort_values('fetched_at', ascending=False)
            combined = combined.head(10000)  # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ 10–∫ –∑–∞–ø–∏—Å—ñ–≤
            combined.to_csv(file_path, index=False)
            print(f"‚úÖ Updated TVL data: {len(df)} new rows")
        else:
            df.to_csv(file_path, index=False)
            print(f"‚úÖ Created TVL data file: {len(df)} rows")

def main():
    """
    –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è - –∑–±–∏—Ä–∞—î –≤—Å—ñ –¥–∞–Ω—ñ Extended –±—ñ—Ä–∂—ñ
    """
    print("üöÄ Starting Extended Exchange data collection...")
    ensure_uploads_dir()
    
    all_markets_data = []
    trading_stats = []
    
    # === 1. –ó–ë–ò–†–ê–Ñ–ú–û –î–ê–ù–Ü –†–ò–ù–ö–Ü–í ===
    for chain in ['ethereum', 'starknet']:
        markets = fetch_markets_data(chain)
        if markets:
            normalized = normalize_markets_data(markets, chain)
            all_markets_data.extend(normalized)
    
    if all_markets_data:
        markets_df = pd.DataFrame(all_markets_data)
        save_markets_data(markets_df)
        print(f"üìä Processed {len(all_markets_data)} market records")
    
    # === 2. –ó–ë–ò–†–ê–Ñ–ú–û –°–¢–ê–¢–ò–°–¢–ò–ö–£ –¢–û–†–ì–Ü–í ===
    # –ó–±–∏—Ä–∞—î–º–æ –¥–∞–Ω—ñ –∑–∞ –æ—Å—Ç–∞–Ω–Ω—ñ 7 –¥–Ω—ñ–≤
    for days_back in range(7):
        date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')
        
        for chain in ['ethereum', 'starknet']:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ü—è –º–µ—Ä–µ–∂–∞ –±—É–ª–∞ –∞–∫—Ç–∏–≤–Ω–∞ –Ω–∞ —Ç—É –¥–∞—Ç—É
            start_date = datetime.strptime(ENDPOINTS[chain]['start_date'], '%Y-%m-%d')
            check_date = datetime.strptime(date, '%Y-%m-%d')
            
            if check_date >= start_date:
                stats = fetch_trading_stats(chain, date)
                trading_stats.append(stats)
    
    if trading_stats:
        save_trading_stats(trading_stats)
    
    # === 3. –ó–ë–ò–†–ê–Ñ–ú–û TVL –î–ê–ù–Ü ===
    tvl_data = fetch_tvl_data()
    if tvl_data:
        save_tvl_data(tvl_data)
    
    print("‚úÖ Data collection completed!")
    print("üìÅ Check uploads/ directory for CSV files")

if __name__ == "__main__":
    main()
